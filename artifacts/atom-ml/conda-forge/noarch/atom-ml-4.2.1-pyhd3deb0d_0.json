{
 "about": {
  "channels": [
   "conda-forge",
   "defaults"
  ],
  "conda_build_version": "3.20.5",
  "conda_private": false,
  "conda_version": "4.9.2",
  "description": "There is no magic formula in data science that can tell us which type of\nmachine learning algorithm will perform best for a specific use-case.\nDifferent models are better suited for different types of data and\ndifferent problems. At best, you can follow some rough guide\non how to approach problems with regard to which model to try on your\ndata, but these are often more confusing than helpful. Best practices\ntell us to start with a simple model (e.g. linear regression) and build\nup to more complicated models (e.g. linear regression -> random forest\n-> multi-layer perceptron) if you are not satisfied with the results.\nUnfortunately, different models require different data cleaning steps,\ndifferent type/amount of features, tuning a new set of hyperparameters,\netc. Refactoring the code for this purpose can be quite boring and\ntime-consuming. Because of this, many data scientists end up just using\nthe model best known to them and fine-tuning this particular model\nwithout ever trying different ones. This can result in poor performance\n(because the model is just not the right one for the task) or in poor\ntime management (because you could have achieved a similar performance\nwith a simpler/faster model).\n\nATOM is made to help you solve these issues. With just a few lines of code,\nyou can perform basic data cleaning steps, select relevant features and\ncompare the performance of multiple models on a given dataset. ATOM should\nbe able to provide quick insights on which algorithms perform best for the\ntask at hand and provide an indication of the feasibility of the ML solution.\n\nIt is important to realize that ATOM is not here to replace all the work a\ndata scientist has to do before getting his model into production. ATOM\ndoesn't spit out production-ready models just by tuning some parameters in\nits API. After helping you to determine the right model, you will most\nprobably need to fine-tune it using use-case specific features and data\ncleaning steps in order to achieve maximum performance.\n\nSo, this sounds a bit like AutoML, how is ATOM different than\nauto-sklearn or TPOT? Well, ATOM does AutoML in\nthe sense that it helps you find the best model for a specific task, but\ncontrary to the aforementioned packages, it does not actively search for\nthe best model. It simply runs all of them and let you pick the one that\nyou think suites the task best. AutoML packages are often black boxes: if\nyou provide data, it will magically return a working model. Although it\nworks great, they often produce complicated pipelines with low explainability.\nThis is hard to sell to the business. In this, ATOM excels. Every step of\nthe pipeline is accounted for, and using the provided plotting methods,\nit\u2019s easy to demonstrate why a model is a better or worse choice than the other.\n",
  "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
  "doc_source_url": "http://github.com/tvdboom/ATOM/docs_sources",
  "doc_url": "http://github.com/tvdboom/ATOM/docs",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "home": "http://github.com/tvdboom/ATOM",
  "identifiers": [],
  "keywords": [],
  "license": "MIT",
  "license_file": "LICENSE",
  "root_pkgs": [
   "liblief 0.10.1 he1b5a44_2",
   "readline 8.0 he28a2e2_2",
   "brotlipy 0.7.0 py38h8df0ef7_1001",
   "yaml 0.2.5 h516909a_0",
   "tk 8.6.10 h21135ba_1",
   "bzip2 1.0.8 h7f98852_4",
   "attrs 20.3.0 pyhd3deb0d_0",
   "git 2.29.2 pl5320h78be0e9_1",
   "pycosat 0.6.3 py38h8df0ef7_1005",
   "beautifulsoup4 4.9.3 pyhb0f4dca_0",
   "pyyaml 5.3.1 py38h8df0ef7_1",
   "python 3.8.6 hffdb5ce_4_cpython",
   "pytz 2020.5 pyhd8ed1ab_0",
   "lz4-c 1.9.2 he1b5a44_3",
   "jsonschema 3.2.0 py_2",
   "curl 7.71.1 he644dc0_8",
   "python_abi 3.8 1_cp38",
   "py-lief 0.10.1 py38h348cfbe_2",
   "ld_impl_linux-64 2.35.1 hea4e1c9_1",
   "libxml2 2.9.10 h72842e0_3",
   "libiconv 1.16 h516909a_0",
   "conda-package-handling 1.7.2 py38h8df0ef7_0",
   "importlib-metadata 3.3.0 py38h578d9bd_2",
   "glob2 0.7 py_0",
   "psutil 5.8.0 py38h497a2fe_0",
   "pkginfo 1.6.1 pyh9f0ad1d_0",
   "krb5 1.17.2 h926e7f8_0",
   "zlib 1.2.11 h516909a_1010",
   "perl 5.32.0 h36c2ea0_0",
   "wheel 0.36.2 pyhd3deb0d_0",
   "six 1.15.0 pyh9f0ad1d_0",
   "jinja2 2.11.2 pyh9f0ad1d_0",
   "ca-certificates 2020.12.5 ha878542_0",
   "libgcc-ng 9.3.0 h5dbcf3e_17",
   "patchelf 0.11 he1b5a44_0",
   "setuptools 49.6.0 py38h924ce5b_2",
   "soupsieve 2.0.1 py_1",
   "markupsafe 1.1.1 py38h8df0ef7_2",
   "icu 68.1 h58526e2_0",
   "pysocks 1.7.1 py38h924ce5b_2",
   "lzo 2.10 h516909a_1000",
   "ncurses 6.2 h58526e2_4",
   "cffi 1.14.4 py38ha65f79e_1",
   "ruamel_yaml 0.15.80 py38h8df0ef7_1003",
   "importlib_metadata 3.3.0 hd8ed1ab_2",
   "tqdm 4.55.0 pyhd8ed1ab_0",
   "c-ares 1.17.1 h36c2ea0_0",
   "nbformat 5.0.8 py_0",
   "gettext 0.19.8.1 h0b5b191_1005",
   "cryptography 3.3.1 py38h2b97feb_0",
   "requests 2.25.1 pyhd3deb0d_0",
   "traitlets 5.0.5 py_0",
   "xz 5.2.5 h516909a_1",
   "openssl 1.1.1i h7f98852_0",
   "pycparser 2.20 pyh9f0ad1d_2",
   "libstdcxx-ng 9.3.0 h2ae2ef3_17",
   "pip 20.3.3 pyhd8ed1ab_0",
   "idna 2.10 pyh9f0ad1d_0",
   "urllib3 1.26.2 pyhd8ed1ab_0",
   "libnghttp2 1.41.0 h8cfc5f6_2",
   "libarchive 3.5.1 h899b81a_0",
   "pyopenssl 20.0.1 pyhd8ed1ab_0",
   "ripgrep 12.1.1 h516909a_1",
   "certifi 2020.12.5 py38h578d9bd_0",
   "conda-build 3.20.5 py38h924ce5b_0",
   "expat 2.2.9 he1b5a44_2",
   "su-exec 0.2 h516909a_1002",
   "filelock 3.0.12 pyh9f0ad1d_0",
   "libcurl 7.71.1 hcdd3856_8",
   "sqlite 3.34.0 h74cdb3f_0",
   "_openmp_mutex 4.5 1_gnu",
   "_libgcc_mutex 0.1 conda_forge",
   "anaconda-client 1.7.2 py_0",
   "libffi 3.3 h58526e2_2",
   "patch 2.7.6 h516909a_1001",
   "libev 4.33 h516909a_1",
   "python-dateutil 2.8.1 py_0",
   "python-libarchive-c 2.9 py38h924ce5b_2",
   "jupyter_core 4.7.0 py38h578d9bd_0",
   "libssh2 1.9.0 hab1572f_5",
   "zipp 3.4.0 py_0",
   "tini 0.18.0 h14c3975_1001",
   "pyrsistent 0.17.3 py38h25fe258_1",
   "libedit 3.1.20191231 he28a2e2_2",
   "pcre 8.44 he1b5a44_0",
   "chardet 4.0.0 py38h578d9bd_0",
   "conda 4.9.2 py38h578d9bd_0",
   "ipython_genutils 0.2.0 py_1",
   "clyent 1.2.2 py_1",
   "zstd 1.4.5 h6597ccf_2",
   "libgomp 9.3.0 h5dbcf3e_17",
   "conda-forge-ci-setup 3.6.1 py38h7bc8238_0",
   "shyaml 0.6.2 pyhd3deb0d_0",
   "click 7.1.2 pyh9f0ad1d_0",
   "oniguruma 6.9.3 h36c2ea0_0",
   "jq 1.6 h36c2ea0_1000",
   "conda-env 2.6.0 1"
  ],
  "summary": "A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines.",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "c_compiler": "gcc",
  "cdt_name": "cos6",
  "channel_sources": "conda-forge,defaults",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "docker_image": "quay.io/condaforge/linux-anvil-comp7",
  "extend_keys": [
   "ignore_build_only_deps",
   "pin_run_as_build",
   "extend_keys",
   "ignore_version"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "numpy",
   "python"
  ],
  "lua": "5",
  "numpy": "1.11",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.6.* *_cpython",
  "r_base": "3.5",
  "target_platform": "linux-64"
 },
 "files": [
  "site-packages/atom/__init__.py",
  "site-packages/atom/api.py",
  "site-packages/atom/atom.py",
  "site-packages/atom/basemodel.py",
  "site-packages/atom/basepredictor.py",
  "site-packages/atom/basetrainer.py",
  "site-packages/atom/basetransformer.py",
  "site-packages/atom/branch.py",
  "site-packages/atom/data_cleaning.py",
  "site-packages/atom/ensembles.py",
  "site-packages/atom/feature_engineering.py",
  "site-packages/atom/modeloptimizer.py",
  "site-packages/atom/models.py",
  "site-packages/atom/plots.py",
  "site-packages/atom/training.py",
  "site-packages/atom/utils.py",
  "site-packages/atom_ml-4.2.1.dist-info/INSTALLER",
  "site-packages/atom_ml-4.2.1.dist-info/LICENSE",
  "site-packages/atom_ml-4.2.1.dist-info/METADATA",
  "site-packages/atom_ml-4.2.1.dist-info/RECORD",
  "site-packages/atom_ml-4.2.1.dist-info/REQUESTED",
  "site-packages/atom_ml-4.2.1.dist-info/WHEEL",
  "site-packages/atom_ml-4.2.1.dist-info/direct_url.json"
 ],
 "index": {
  "arch": null,
  "build": "pyhd3deb0d_0",
  "build_number": 0,
  "depends": [
   "category_encoders",
   "featuretools",
   "gplearn",
   "imbalanced-learn",
   "joblib",
   "matplotlib-base",
   "numpy",
   "pandas",
   "pandas-profiling",
   "python >=3.6",
   "scikit-learn",
   "scikit-optimize",
   "scipy",
   "seaborn",
   "shap",
   "tabulate",
   "tqdm",
   "typeguard"
  ],
  "license": "MIT",
  "name": "atom-ml",
  "noarch": "python",
  "platform": null,
  "subdir": "noarch",
  "timestamp": 1609237994366,
  "version": "4.2.1"
 },
 "metadata_version": 1,
 "name": "atom-ml",
 "raw_recipe": "{% set name = \"atom-ml\" %}\n{% set version = \"4.2.1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 7e59bc40e648fd7da3c03d43352519721e287ef4afe15832e187e062a5cb600a\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . --no-deps -vv\n\nrequirements:\n  host:\n    - python >=3.6\n    - pip\n  run:\n    - python >=3.6\n    - numpy\n    - scipy\n    - pandas\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn\n    - scikit-optimize\n    - category_encoders\n    - imbalanced-learn\n    - pandas-profiling\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n\ntest:\n  requires:\n    - pip\n    - pytest\n    - coverage\n    - python\n    - tensorflow\n    - keras\n    - numpy\n    - scipy\n    - pandas\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn\n    - scikit-optimize\n    - category_encoders\n    - imbalanced-learn\n    - pandas-profiling\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n    - py-xgboost  # [not win]\n    - lightgbm\n    - catboost\n  source_files:\n    - tests\n  imports:\n    - atom\n  commands:\n    - mkdir tests/files  # Directory to save files during testing\n    - coverage run -m pytest\n\nabout:\n  home: http://github.com/tvdboom/ATOM\n  license: MIT\n  license_file: LICENSE\n  summary: A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines.\n  description: |\n    There is no magic formula in data science that can tell us which type of\n    machine learning algorithm will perform best for a specific use-case.\n    Different models are better suited for different types of data and\n    different problems. At best, you can follow some rough guide\n    on how to approach problems with regard to which model to try on your\n    data, but these are often more confusing than helpful. Best practices\n    tell us to start with a simple model (e.g. linear regression) and build\n    up to more complicated models (e.g. linear regression -> random forest\n    -> multi-layer perceptron) if you are not satisfied with the results.\n    Unfortunately, different models require different data cleaning steps,\n    different type/amount of features, tuning a new set of hyperparameters,\n    etc. Refactoring the code for this purpose can be quite boring and\n    time-consuming. Because of this, many data scientists end up just using\n    the model best known to them and fine-tuning this particular model\n    without ever trying different ones. This can result in poor performance\n    (because the model is just not the right one for the task) or in poor\n    time management (because you could have achieved a similar performance\n    with a simpler/faster model).\n\n    ATOM is made to help you solve these issues. With just a few lines of code,\n    you can perform basic data cleaning steps, select relevant features and\n    compare the performance of multiple models on a given dataset. ATOM should\n    be able to provide quick insights on which algorithms perform best for the\n    task at hand and provide an indication of the feasibility of the ML solution.\n\n    It is important to realize that ATOM is not here to replace all the work a\n    data scientist has to do before getting his model into production. ATOM\n    doesn't spit out production-ready models just by tuning some parameters in\n    its API. After helping you to determine the right model, you will most\n    probably need to fine-tune it using use-case specific features and data\n    cleaning steps in order to achieve maximum performance.\n\n    So, this sounds a bit like AutoML, how is ATOM different than \n    auto-sklearn or TPOT? Well, ATOM does AutoML in\n    the sense that it helps you find the best model for a specific task, but\n    contrary to the aforementioned packages, it does not actively search for\n    the best model. It simply runs all of them and let you pick the one that\n    you think suites the task best. AutoML packages are often black boxes: if\n    you provide data, it will magically return a working model. Although it\n    works great, they often produce complicated pipelines with low explainability.\n    This is hard to sell to the business. In this, ATOM excels. Every step of\n    the pipeline is accounted for, and using the provided plotting methods,\n    it\u2019s easy to demonstrate why a model is a better or worse choice than the other.\n\n  doc_url: http://github.com/tvdboom/ATOM/docs\n  doc_source_url: http://github.com/tvdboom/ATOM/docs_sources\n  dev_url: http://github.com/tvdboom/ATOM/tree/development\n\nextra:\n  recipe-maintainers:\n    - tvdboom\n",
 "rendered_recipe": {
  "about": {
   "description": "There is no magic formula in data science that can tell us which type of\nmachine learning algorithm will perform best for a specific use-case.\nDifferent models are better suited for different types of data and\ndifferent problems. At best, you can follow some rough guide\non how to approach problems with regard to which model to try on your\ndata, but these are often more confusing than helpful. Best practices\ntell us to start with a simple model (e.g. linear regression) and build\nup to more complicated models (e.g. linear regression -> random forest\n-> multi-layer perceptron) if you are not satisfied with the results.\nUnfortunately, different models require different data cleaning steps,\ndifferent type/amount of features, tuning a new set of hyperparameters,\netc. Refactoring the code for this purpose can be quite boring and\ntime-consuming. Because of this, many data scientists end up just using\nthe model best known to them and fine-tuning this particular model\nwithout ever trying different ones. This can result in poor performance\n(because the model is just not the right one for the task) or in poor\ntime management (because you could have achieved a similar performance\nwith a simpler/faster model).\n\nATOM is made to help you solve these issues. With just a few lines of code,\nyou can perform basic data cleaning steps, select relevant features and\ncompare the performance of multiple models on a given dataset. ATOM should\nbe able to provide quick insights on which algorithms perform best for the\ntask at hand and provide an indication of the feasibility of the ML solution.\n\nIt is important to realize that ATOM is not here to replace all the work a\ndata scientist has to do before getting his model into production. ATOM\ndoesn't spit out production-ready models just by tuning some parameters in\nits API. After helping you to determine the right model, you will most\nprobably need to fine-tune it using use-case specific features and data\ncleaning steps in order to achieve maximum performance.\n\nSo, this sounds a bit like AutoML, how is ATOM different than\nauto-sklearn or TPOT? Well, ATOM does AutoML in\nthe sense that it helps you find the best model for a specific task, but\ncontrary to the aforementioned packages, it does not actively search for\nthe best model. It simply runs all of them and let you pick the one that\nyou think suites the task best. AutoML packages are often black boxes: if\nyou provide data, it will magically return a working model. Although it\nworks great, they often produce complicated pipelines with low explainability.\nThis is hard to sell to the business. In this, ATOM excels. Every step of\nthe pipeline is accounted for, and using the provided plotting methods,\nit\u2019s easy to demonstrate why a model is a better or worse choice than the other.\n",
   "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
   "doc_source_url": "http://github.com/tvdboom/ATOM/docs_sources",
   "doc_url": "http://github.com/tvdboom/ATOM/docs",
   "home": "http://github.com/tvdboom/ATOM",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/home/conda/feedstock_root/build_artifacts/atom-ml_1609237776689/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . --no-deps -vv",
   "string": "pyhd3deb0d_0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "package": {
   "name": "atom-ml",
   "version": "4.2.1"
  },
  "requirements": {
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 1_gnu",
    "ca-certificates 2020.12.5 ha878542_0",
    "certifi 2020.12.5 py39hf3d152e_0",
    "ld_impl_linux-64 2.35.1 hea4e1c9_1",
    "libffi 3.3 h58526e2_2",
    "libgcc-ng 9.3.0 h5dbcf3e_17",
    "libgomp 9.3.0 h5dbcf3e_17",
    "libstdcxx-ng 9.3.0 h2ae2ef3_17",
    "ncurses 6.2 h58526e2_4",
    "openssl 1.1.1i h7f98852_0",
    "pip 20.3.3 pyhd8ed1ab_0",
    "python 3.9.1 hffdb5ce_2_cpython",
    "python_abi 3.9 1_cp39",
    "readline 8.0 he28a2e2_2",
    "setuptools 49.6.0 py39h079e4ff_2",
    "sqlite 3.34.0 h74cdb3f_0",
    "tk 8.6.10 h21135ba_1",
    "tzdata 2020e he74cb21_0",
    "wheel 0.36.2 pyhd3deb0d_0",
    "xz 5.2.5 h516909a_1",
    "zlib 1.2.11 h516909a_1010"
   ],
   "run": [
    "category_encoders",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python >=3.6",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  },
  "source": {
   "sha256": "7e59bc40e648fd7da3c03d43352519721e287ef4afe15832e187e062a5cb600a",
   "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.2.1.tar.gz"
  },
  "test": {
   "commands": [
    "mkdir tests/files",
    "coverage run -m pytest"
   ],
   "imports": [
    "atom"
   ],
   "requires": [
    "catboost",
    "category_encoders",
    "coverage",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "keras",
    "lightgbm",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tensorflow",
    "tqdm",
    "typeguard"
   ],
   "source_files": [
    "tests"
   ]
  }
 },
 "version": "4.2.1"
}