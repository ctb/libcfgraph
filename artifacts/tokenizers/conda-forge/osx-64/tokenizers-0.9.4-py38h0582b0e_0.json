{
 "about": {
  "channels": [
   "conda-forge",
   "defaults"
  ],
  "conda_build_version": "3.20.5",
  "conda_private": false,
  "conda_version": "4.9.2",
  "dev_url": "https://github.com/huggingface/tokenizers",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "rluria14",
    "setu4993"
   ]
  },
  "home": "https://pypi.org/project/tokenizers/",
  "identifiers": [],
  "keywords": [],
  "license": "Apache-2.0",
  "license_family": "APACHE",
  "license_file": "LICENSE",
  "root_pkgs": [
   "ruamel_yaml 0.15.80 py38h94c058a_1003",
   "urllib3 1.25.11 py_0",
   "conda-env 2.6.0 1",
   "markupsafe 1.1.1 py38h94c058a_2",
   "pip 20.2.4 py_0",
   "requests 2.25.0 pyhd3deb0d_0",
   "tk 8.6.10 hb0a8c7a_1",
   "idna 2.10 pyh9f0ad1d_0",
   "py-lief 0.10.1 py38heedec30_2",
   "ncurses 6.2 h2e338ed_4",
   "jq 1.6 h1de35cc_1000",
   "nbformat 5.0.8 py_0",
   "anaconda-client 1.7.2 py_0",
   "libcurl 7.71.1 h9bf37e3_8",
   "jsonschema 3.2.0 py_2",
   "sqlite 3.33.0 h960bd1c_1",
   "libffi 3.2.1 hb1e8313_1007",
   "cryptography 3.2.1 py38h5c1d3f9_0",
   "pycparser 2.20 pyh9f0ad1d_2",
   "soupsieve 2.0.1 py_1",
   "libxml2 2.9.10 h2c6e4a5_2",
   "beautifulsoup4 4.9.3 pyhb0f4dca_0",
   "glob2 0.7 py_0",
   "shyaml 0.6.1 py_0",
   "perl 5.32.0 hbcb3906_0",
   "libedit 3.1.20191231 h0678c8f_2",
   "openssl 1.1.1h haf1e3a3_0",
   "traitlets 5.0.5 py_0",
   "libiconv 1.16 haf1e3a3_0",
   "expat 2.2.9 hb1e8313_2",
   "lz4-c 1.9.2 hb1e8313_3",
   "xz 5.2.5 haf1e3a3_1",
   "liblief 0.10.1 hb1e8313_2",
   "conda-forge-ci-setup 3.5.0 py38he866dac_0",
   "brotlipy 0.7.0 py38h94c058a_1001",
   "git 2.29.2 pl5320h422953c_1",
   "zlib 1.2.11 h7795811_1010",
   "pcre 8.44 hb1e8313_0",
   "python 3.8.6 hcfdab8c_0_cpython",
   "wheel 0.35.1 pyh9f0ad1d_0",
   "importlib-metadata 2.0.0 py_1",
   "curl 7.71.1 hcb81553_8",
   "tqdm 4.52.0 pyhd3deb0d_0",
   "click 7.1.2 pyh9f0ad1d_0",
   "conda-build 3.20.5 py38h5347e94_0",
   "zstd 1.4.5 h289c70a_2",
   "cffi 1.14.3 py38h9edaa1b_1",
   "attrs 20.3.0 pyhd3deb0d_0",
   "python_abi 3.8 1_cp38",
   "libssh2 1.9.0 h8a08a2b_5",
   "pkginfo 1.6.1 pyh9f0ad1d_0",
   "readline 8.0 h0678c8f_2",
   "pyopenssl 19.1.0 py_1",
   "icu 67.1 hb1e8313_0",
   "ripgrep 12.1.1 haf1e3a3_1",
   "conda 4.9.2 py38h50d1736_0",
   "c-ares 1.16.1 haf1e3a3_3",
   "pycosat 0.6.3 py38h94c058a_1005",
   "yaml 0.2.5 haf1e3a3_0",
   "bzip2 1.0.8 haf1e3a3_3",
   "ca-certificates 2020.11.8 h033912b_0",
   "pytz 2020.4 pyhd8ed1ab_0",
   "jinja2 2.11.2 pyh9f0ad1d_0",
   "certifi 2020.11.8 py38h50d1736_0",
   "clyent 1.2.2 py_1",
   "gettext 0.19.8.1 haf92f58_1004",
   "libarchive 3.4.3 hf12134e_0",
   "conda-package-handling 1.7.2 py38h94c058a_0",
   "pysocks 1.7.1 py38h5347e94_2",
   "zipp 3.4.0 py_0",
   "chardet 3.0.4 py38h5347e94_1008",
   "psutil 5.7.3 py38h94c058a_0",
   "importlib_metadata 2.0.0 1",
   "pyrsistent 0.17.3 py38h7e3306e_1",
   "pyyaml 5.3.1 py38h94c058a_1",
   "python-libarchive-c 2.9 py38h5347e94_2",
   "libnghttp2 1.41.0 h7580e61_2",
   "ipython_genutils 0.2.0 py_1",
   "krb5 1.17.2 h60d9502_0",
   "libcxx 11.0.0 h439d374_0",
   "lzo 2.10 haf1e3a3_1000",
   "oniguruma 6.9.3 h01d97ff_0",
   "python-dateutil 2.8.1 py_0",
   "six 1.15.0 pyh9f0ad1d_0",
   "libev 4.33 haf1e3a3_1",
   "filelock 3.0.12 pyh9f0ad1d_0",
   "jupyter_core 4.7.0 py38h50d1736_0",
   "setuptools 49.6.0 py38h5347e94_2"
  ],
  "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "CONDA_BUILD_SYSROOT": "/Applications/Xcode_12.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.9.sdk",
  "MACOSX_DEPLOYMENT_TARGET": "10.9",
  "c_compiler": "clang",
  "channel_sources": "conda-forge,defaults",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "clangxx",
  "cxx_compiler_version": "11",
  "extend_keys": [
   "ignore_version",
   "ignore_build_only_deps",
   "pin_run_as_build",
   "extend_keys"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "macos_machine": "x86_64-apple-darwin13.4.0",
  "numpy": "1.11",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.8.* *_cpython",
  "r_base": "3.5",
  "rust_compiler": "rust",
  "target_platform": "osx-64"
 },
 "files": [
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/INSTALLER",
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/METADATA",
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/RECORD",
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/REQUESTED",
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/WHEEL",
  "lib/python3.8/site-packages/tokenizers-0.9.4.dist-info/direct_url.json",
  "lib/python3.8/site-packages/tokenizers/__init__.py",
  "lib/python3.8/site-packages/tokenizers/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/decoders/__init__.py",
  "lib/python3.8/site-packages/tokenizers/decoders/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/implementations/__init__.py",
  "lib/python3.8/site-packages/tokenizers/implementations/base_tokenizer.py",
  "lib/python3.8/site-packages/tokenizers/implementations/bert_wordpiece.py",
  "lib/python3.8/site-packages/tokenizers/implementations/byte_level_bpe.py",
  "lib/python3.8/site-packages/tokenizers/implementations/char_level_bpe.py",
  "lib/python3.8/site-packages/tokenizers/implementations/sentencepiece_bpe.py",
  "lib/python3.8/site-packages/tokenizers/implementations/sentencepiece_unigram.py",
  "lib/python3.8/site-packages/tokenizers/models/__init__.py",
  "lib/python3.8/site-packages/tokenizers/models/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/normalizers/__init__.py",
  "lib/python3.8/site-packages/tokenizers/normalizers/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/pre_tokenizers/__init__.py",
  "lib/python3.8/site-packages/tokenizers/pre_tokenizers/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/processors/__init__.py",
  "lib/python3.8/site-packages/tokenizers/processors/__init__.pyi",
  "lib/python3.8/site-packages/tokenizers/tokenizers.cpython-38-darwin.so",
  "lib/python3.8/site-packages/tokenizers/trainers/__init__.py",
  "lib/python3.8/site-packages/tokenizers/trainers/__init__.pyi"
 ],
 "index": {
  "arch": "x86_64",
  "build": "py38h0582b0e_0",
  "build_number": 0,
  "depends": [
   "libcxx >=11.0.0",
   "python >=3.8,<3.9.0a0",
   "python_abi 3.8.* *_cp38"
  ],
  "license": "Apache-2.0",
  "license_family": "APACHE",
  "name": "tokenizers",
  "platform": "osx",
  "subdir": "osx-64",
  "timestamp": 1605739106884,
  "version": "0.9.4"
 },
 "metadata_version": 1,
 "name": "tokenizers",
 "raw_recipe": "{% set name = \"tokenizers\" %}\n{% set version = \"0.9.4\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 3ea3038008f1f74c8a1e1e2e73728690eed2d7fa4db0a51bcea391e644672426\n\nbuild:\n  number: 0\n  missing_dso_whitelist:\n    - /usr/lib/libresolv.9.dylib  # [osx]\n    - /usr/lib64/libgcc_s.so.1  # [linux]\n  script:\n    - {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  build:\n    - {{ compiler('cxx') }}\n    - {{ compiler('rust') }}\n  host:\n    - python\n    - pip\n    - setuptools-rust >=0.11.5\n    - setuptools\n  run:\n    - python\n\ntest:\n  imports:\n    - tokenizers\n    - tokenizers.models\n    - tokenizers.decoders\n    - tokenizers.normalizers\n    - tokenizers.pre_tokenizers\n    - tokenizers.processors\n    - tokenizers.trainers\n    - tokenizers.implementations\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://pypi.org/project/tokenizers/\n  license: Apache-2.0\n  license_family: APACHE\n  license_file: LICENSE\n  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production\n  dev_url: https://github.com/huggingface/tokenizers\n\nextra:\n  recipe-maintainers:\n    - anthchirp\n    - ndmaxar\n    - oblute\n    - rluria14\n    - setu4993\n",
 "rendered_recipe": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib/libresolv.9.dylib"
   ],
   "number": "0",
   "script": [
    "/Users/runner/miniforge3/conda-bld/tokenizers_1605738551056/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/bin/python -m pip install . -vv"
   ],
   "string": "py38h0582b0e_0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "rluria14",
    "setu4993"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.9.4"
  },
  "requirements": {
   "build": [
    "cctools_osx-64 949.0.1 h8e49ea9_16",
    "clang 11.0.0 1",
    "clang-11 11.0.0 default_h000dee7_1",
    "clang_osx-64 11.0.0 hb91bd55_5",
    "clangxx 11.0.0 default_hf57f61e_1",
    "clangxx_osx-64 11.0.0 h7e1b574_5",
    "compiler-rt 11.0.0 h01488ec_2",
    "compiler-rt_osx-64 11.0.0 hd3c4e95_2",
    "ld64_osx-64 530 h66d9e41_16",
    "ldid 2.1.2 h7660a38_2",
    "libclang-cpp11 11.0.0 default_h000dee7_1",
    "libcxx 11.0.0 h439d374_0",
    "libllvm11 11.0.0 hf85e3d2_0",
    "llvm-tools 11.0.0 hf85e3d2_0",
    "rust 1.46.0 1",
    "rust-std-x86_64-apple-darwin 1.46.0 h92ee837_1",
    "rust_osx-64 1.46.0 hb4dc7ee_0",
    "tapi 1100.0.11 h9ce4665_0",
    "zlib 1.2.11 h7795811_1010"
   ],
   "host": [
    "ca-certificates 2020.11.8 h033912b_0",
    "certifi 2020.11.8 py38h50d1736_0",
    "libcxx 11.0.0 h439d374_0",
    "libffi 3.2.1 hb1e8313_1007",
    "ncurses 6.2 h2e338ed_4",
    "openssl 1.1.1h haf1e3a3_0",
    "pip 20.2.4 py_0",
    "python 3.8.6 hcfdab8c_0_cpython",
    "python_abi 3.8 1_cp38",
    "readline 8.0 h0678c8f_2",
    "semantic_version 2.8.5 pyh9f0ad1d_0",
    "setuptools 49.6.0 py38h5347e94_2",
    "setuptools-rust 0.11.5 pyhd8ed1ab_0",
    "sqlite 3.33.0 h960bd1c_1",
    "tk 8.6.10 hb0a8c7a_1",
    "toml 0.10.2 pyhd8ed1ab_0",
    "wheel 0.35.1 pyh9f0ad1d_0",
    "xz 5.2.5 haf1e3a3_1",
    "zlib 1.2.11 h7795811_1010"
   ],
   "run": [
    "libcxx >=11.0.0",
    "python >=3.8,<3.9.0a0",
    "python_abi 3.8.* *_cp38"
   ]
  },
  "source": {
   "sha256": "3ea3038008f1f74c8a1e1e2e73728690eed2d7fa4db0a51bcea391e644672426",
   "url": "https://pypi.io/packages/source/t/tokenizers/tokenizers-0.9.4.tar.gz"
  },
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "tokenizers",
    "tokenizers.decoders",
    "tokenizers.implementations",
    "tokenizers.models",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "version": "0.9.4"
}