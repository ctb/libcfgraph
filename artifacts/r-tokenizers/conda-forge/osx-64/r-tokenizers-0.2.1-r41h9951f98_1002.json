{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge",
   "https://repo.anaconda.com/pkgs/main"
  ],
  "conda_build_version": "3.21.4",
  "conda_private": false,
  "conda_version": "4.10.1",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "conda-forge/r"
   ]
  },
  "home": "https://lincolnmullen.com/software/tokenizers/",
  "identifiers": [],
  "keywords": [],
  "license": "MIT",
  "license_family": "MIT",
  "license_file": [
   "/Users/runner/miniforge3/conda-bld/r-tokenizers_1622089083337/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/lib/R/share/licenses/MIT",
   "LICENSE"
  ],
  "root_pkgs": [
   "python_abi 3.9 1_cp39",
   "nbformat 5.1.3 pyhd8ed1ab_0",
   "ca-certificates 2020.12.5 h033912b_0",
   "conda-env 2.6.0 1",
   "six 1.16.0 pyh6c4a22f_0",
   "c-ares 1.17.1 h0d85af4_1",
   "idna 2.10 pyh9f0ad1d_0",
   "jq 1.6 hc929b4f_1000",
   "ncurses 6.2 h2e338ed_4",
   "ruamel_yaml 0.15.80 py39h4b0b724_1004",
   "jinja2 3.0.1 pyhd8ed1ab_0",
   "wheel 0.36.2 pyhd3deb0d_0",
   "pyyaml 5.4.1 py39hcbf5805_0",
   "pycparser 2.20 pyh9f0ad1d_2",
   "attrs 21.2.0 pyhd8ed1ab_0",
   "krb5 1.19.1 hcfbf3a7_0",
   "soupsieve 2.0.1 py_1",
   "pip 21.1.2 pyhd8ed1ab_0",
   "jupyter_core 4.7.1 py39h6e9494a_0",
   "requests 2.25.1 pyhd3deb0d_0",
   "liblief 0.11.5 he49afe7_0",
   "beautifulsoup4 4.9.3 pyhb0f4dca_0",
   "glob2 0.7 py_0",
   "cffi 1.14.5 py39h319c39b_0",
   "jsonschema 3.2.0 pyhd8ed1ab_3",
   "importlib-metadata 4.2.0 py39h6e9494a_0",
   "perl 5.32.0 hbcb3906_0",
   "libedit 3.1.20191231 h0678c8f_2",
   "cryptography 3.4.7 py39ha2c9959_0",
   "traitlets 5.0.5 py_0",
   "libiconv 1.16 haf1e3a3_0",
   "sqlite 3.35.5 h44b9ce1_0",
   "click 8.0.1 py39h6e9494a_0",
   "tk 8.6.10 h0419947_1",
   "xz 5.2.5 haf1e3a3_1",
   "tzdata 2021a he74cb21_0",
   "zipp 3.4.1 pyhd8ed1ab_0",
   "psutil 5.8.0 py39h89e85a6_1",
   "zlib 1.2.11 h7795811_1010",
   "pcre 8.44 hb1e8313_0",
   "libffi 3.3 h046ec9c_2",
   "markupsafe 2.0.1 py39h89e85a6_0",
   "git 2.30.2 pl5320h2551975_1",
   "libcxx 11.1.0 habf9029_0",
   "gettext 0.19.8.1 h7937167_1005",
   "py-lief 0.11.5 py39h9fcab8e_0",
   "pyopenssl 20.0.1 pyhd8ed1ab_0",
   "chardet 4.0.0 py39h6e9494a_1",
   "conda-package-handling 1.7.3 py39h89e85a6_0",
   "conda-build 3.21.4 py39h6e9494a_0",
   "pysocks 1.7.1 py39h6e9494a_3",
   "urllib3 1.26.4 pyhd8ed1ab_0",
   "python-libarchive-c 3.0 py39h6e9494a_0",
   "python 3.9.4 h9133fd0_0_cpython",
   "pkginfo 1.7.0 pyhd8ed1ab_0",
   "openssl 1.1.1k h0d85af4_0",
   "zstd 1.5.0 h582d3a0_0",
   "conda 4.10.1 py39h6e9494a_0",
   "ripgrep 12.1.1 haf1e3a3_1",
   "pycosat 0.6.3 py39hcbf5805_1006",
   "tqdm 4.60.0 pyhd8ed1ab_0",
   "yaml 0.2.5 haf1e3a3_0",
   "curl 7.77.0 hb861fe1_0",
   "pytz 2021.1 pyhd8ed1ab_0",
   "readline 8.1 h05e3726_0",
   "libxml2 2.9.12 h93ec3fd_0",
   "brotlipy 0.7.0 py39hcbf5805_1001",
   "setuptools 49.6.0 py39h6e9494a_3",
   "pyrsistent 0.17.3 py39hcbf5805_2",
   "conda-forge-ci-setup 3.9.4 py39hb0a6171_0",
   "bzip2 1.0.8 h0d85af4_4",
   "anaconda-client 1.7.2 pyhd8ed1ab_1",
   "libcurl 7.77.0 hf45b732_0",
   "shyaml 0.6.2 pyhd3deb0d_0",
   "libnghttp2 1.43.0 h07e645a_0",
   "clyent 1.2.2 py_1",
   "libarchive 3.5.1 h2b60450_2",
   "libssh2 1.9.0 h52ee1ee_6",
   "lz4-c 1.9.3 h046ec9c_0",
   "certifi 2020.12.5 py39h6e9494a_1",
   "ipython_genutils 0.2.0 py_1",
   "lzo 2.10 haf1e3a3_1000",
   "python-dateutil 2.8.1 py_0",
   "expat 2.4.1 he49afe7_0",
   "libev 4.33 haf1e3a3_1",
   "filelock 3.0.12 pyh9f0ad1d_0",
   "oniguruma 6.9.7.1 h0d85af4_0",
   "icu 68.1 h74dc148_0"
  ],
  "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. ",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "CONDA_BUILD_SYSROOT": "/Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.9.sdk",
  "MACOSX_DEPLOYMENT_TARGET": "10.9",
  "c_compiler": "clang",
  "c_compiler_version": "11",
  "channel_sources": "conda-forge,defaults",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "clangxx",
  "cxx_compiler_version": "11",
  "extend_keys": [
   "ignore_build_only_deps",
   "pin_run_as_build",
   "extend_keys",
   "ignore_version"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "numpy",
   "python"
  ],
  "lua": "5",
  "macos_machine": "x86_64-apple-darwin13.4.0",
  "numpy": "1.16",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.9",
  "r_base": "4.1",
  "target_platform": "osx-64",
  "zip_keys": [
   [
    "c_compiler_version",
    "cxx_compiler_version"
   ]
  ]
 },
 "files": [
  "lib/R/library/tokenizers/CITATION",
  "lib/R/library/tokenizers/DESCRIPTION",
  "lib/R/library/tokenizers/INDEX",
  "lib/R/library/tokenizers/LICENSE",
  "lib/R/library/tokenizers/Meta/Rd.rds",
  "lib/R/library/tokenizers/Meta/data.rds",
  "lib/R/library/tokenizers/Meta/features.rds",
  "lib/R/library/tokenizers/Meta/hsearch.rds",
  "lib/R/library/tokenizers/Meta/links.rds",
  "lib/R/library/tokenizers/Meta/nsInfo.rds",
  "lib/R/library/tokenizers/Meta/package.rds",
  "lib/R/library/tokenizers/Meta/vignette.rds",
  "lib/R/library/tokenizers/NAMESPACE",
  "lib/R/library/tokenizers/NEWS.md",
  "lib/R/library/tokenizers/R/tokenizers",
  "lib/R/library/tokenizers/R/tokenizers.rdb",
  "lib/R/library/tokenizers/R/tokenizers.rdx",
  "lib/R/library/tokenizers/data/Rdata.rdb",
  "lib/R/library/tokenizers/data/Rdata.rds",
  "lib/R/library/tokenizers/data/Rdata.rdx",
  "lib/R/library/tokenizers/doc/index.html",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.R",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.Rmd",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.html",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.R",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.Rmd",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.html",
  "lib/R/library/tokenizers/help/AnIndex",
  "lib/R/library/tokenizers/help/aliases.rds",
  "lib/R/library/tokenizers/help/paths.rds",
  "lib/R/library/tokenizers/help/tokenizers.rdb",
  "lib/R/library/tokenizers/help/tokenizers.rdx",
  "lib/R/library/tokenizers/html/00Index.html",
  "lib/R/library/tokenizers/html/R.css",
  "lib/R/library/tokenizers/libs/tokenizers.dylib"
 ],
 "index": {
  "arch": "x86_64",
  "build": "r41h9951f98_1002",
  "build_number": 1002,
  "depends": [
   "libcxx >=11.1.0",
   "r-base >=4.1,<4.2.0a0",
   "r-rcpp >=0.12.3",
   "r-snowballc >=0.5.1",
   "r-stringi >=1.0.1"
  ],
  "license": "MIT",
  "license_family": "MIT",
  "name": "r-tokenizers",
  "platform": "osx",
  "subdir": "osx-64",
  "timestamp": 1622089423118,
  "version": "0.2.1"
 },
 "metadata_version": 1,
 "name": "r-tokenizers",
 "raw_recipe": "{% set version = \"0.2.1\" %}\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-tokenizers\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  fn: tokenizers_{{ version }}.tar.gz\n  url:\n    - {{ cran_mirror }}/src/contrib/tokenizers_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/tokenizers/tokenizers_{{ version }}.tar.gz\n  sha256: 28617cdc5ddef5276abfe14a2642999833322b6c34697de1d4e9d6dc7670dd00\n\nbuild:\n  merge_build_host: true  # [win]\n  number: 1002\n  skip: true  # [win32]\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - {{ compiler('c') }}        # [not win]\n    - {{ compiler('cxx') }}      # [not win]\n    - {{ compiler('m2w64_c') }}        # [win]\n    - {{ compiler('m2w64_cxx') }}        # [win]\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}make\n    - {{ posix }}sed               # [win]\n    - {{ posix }}coreutils         # [win]\n    - {{ posix }}zip               # [win]\n  host:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n  run:\n    - r-base\n    - {{ native }}gcc-libs         # [win]\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n\ntest:\n  commands:\n    - $R -e \"library('tokenizers')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('tokenizers')\\\"\"  # [win]\n\nabout:\n  home: https://lincolnmullen.com/software/tokenizers/\n  license: MIT\n  summary: \"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for\\\n    \\ splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. \"\n  license_family: MIT\n\n  license_file:\n    - {{ environ[\"PREFIX\"] }}/lib/R/share/licenses/MIT\n    - LICENSE\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n",
 "rendered_recipe": {
  "about": {
   "home": "https://lincolnmullen.com/software/tokenizers/",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": [
    "/Users/runner/miniforge3/conda-bld/r-tokenizers_1622089083337/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/lib/R/share/licenses/MIT",
    "LICENSE"
   ],
   "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
  },
  "build": {
   "number": "1002",
   "rpaths": [
    "lib/",
    "lib/R/lib/"
   ],
   "string": "r41h9951f98_1002"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "conda-forge/r"
   ]
  },
  "package": {
   "name": "r-tokenizers",
   "version": "0.2.1"
  },
  "requirements": {
   "build": [
    "cctools_osx-64 949.0.1 h6407bdd_22",
    "clang 11.1.0 h694c41f_1",
    "clang-11 11.1.0 default_he082bbe_1",
    "clang_osx-64 11.1.0 hb91bd55_2",
    "clangxx 11.1.0 default_he082bbe_1",
    "clangxx_osx-64 11.1.0 h7e1b574_2",
    "compiler-rt 11.1.0 h654b07c_0",
    "compiler-rt_osx-64 11.1.0 h8c5fa43_0",
    "ld64_osx-64 530 he8994da_21",
    "ldid 2.1.2 h7660a38_2",
    "libclang-cpp11.1 11.1.0 default_he082bbe_1",
    "libcxx 11.1.0 habf9029_0",
    "libllvm11 11.1.0 hd011deb_2",
    "libllvm12 12.0.0 hd011deb_1",
    "llvm-tools 11.1.0 hd011deb_2",
    "make 4.3 h22f3db7_1",
    "tapi 1100.0.11 h9ce4665_0",
    "zlib 1.2.11 h7795811_1010"
   ],
   "host": [
    "_r-mutex 1.0.1 anacondar_1",
    "bwidget 1.9.14 h694c41f_0",
    "bzip2 1.0.8 h0d85af4_4",
    "c-ares 1.17.1 h0d85af4_1",
    "ca-certificates 2020.12.5 h033912b_0",
    "cairo 1.16.0 he43a7df_1008",
    "cctools_osx-64 949.0.1 h6407bdd_22",
    "clang 11.1.0 h694c41f_1",
    "clang-11 11.1.0 default_he082bbe_1",
    "clang_osx-64 11.1.0 hb91bd55_2",
    "clangxx 11.1.0 default_he082bbe_1",
    "clangxx_osx-64 11.1.0 h7e1b574_2",
    "compiler-rt 11.1.0 h654b07c_0",
    "compiler-rt_osx-64 11.1.0 h8c5fa43_0",
    "curl 7.77.0 hb861fe1_0",
    "font-ttf-dejavu-sans-mono 2.37 hab24e00_0",
    "font-ttf-inconsolata 3.000 h77eed37_0",
    "font-ttf-source-code-pro 2.038 h77eed37_0",
    "font-ttf-ubuntu 0.83 hab24e00_0",
    "fontconfig 2.13.1 h10f422b_1005",
    "fonts-conda-ecosystem 1 0",
    "fonts-conda-forge 1 0",
    "freetype 2.10.4 h4cff582_1",
    "fribidi 1.0.10 hbcb3906_0",
    "gettext 0.19.8.1 h7937167_1005",
    "gfortran_impl_osx-64 9.3.0 h9cc0e5e_22",
    "gfortran_osx-64 9.3.0 h18f7dce_14",
    "gmp 6.2.1 h2e338ed_0",
    "graphite2 1.3.13 h2e338ed_1001",
    "gsl 2.6 h71c5fe9_2",
    "harfbuzz 2.8.1 h159f659_0",
    "icu 68.1 h74dc148_0",
    "isl 0.22.1 hb1e8313_2",
    "jbig 2.1 h0d85af4_2003",
    "jpeg 9d hbcb3906_0",
    "krb5 1.19.1 hcfbf3a7_0",
    "ld64_osx-64 530 he8994da_21",
    "ldid 2.1.2 h7660a38_2",
    "lerc 2.2.1 h046ec9c_0",
    "libblas 3.9.0 9_openblas",
    "libcblas 3.9.0 9_openblas",
    "libclang-cpp11.1 11.1.0 default_he082bbe_1",
    "libcurl 7.77.0 hf45b732_0",
    "libcxx 11.1.0 habf9029_0",
    "libdeflate 1.7 h35c211d_5",
    "libedit 3.1.20191231 h0678c8f_2",
    "libev 4.33 haf1e3a3_1",
    "libffi 3.3 h046ec9c_2",
    "libgfortran 5.0.0 9_3_0_h6c81a4c_22",
    "libgfortran-devel_osx-64 9.3.0 h6c81a4c_22",
    "libgfortran5 9.3.0 h6c81a4c_22",
    "libglib 2.68.2 hd556434_0",
    "libiconv 1.16 haf1e3a3_0",
    "liblapack 3.9.0 9_openblas",
    "libllvm11 11.1.0 hd011deb_2",
    "libllvm12 12.0.0 hd011deb_1",
    "libnghttp2 1.43.0 h07e645a_0",
    "libopenblas 0.3.15 openmp_h5e1b9a4_1",
    "libpng 1.6.37 h7cec526_2",
    "libssh2 1.9.0 h52ee1ee_6",
    "libtiff 4.3.0 h1167814_1",
    "libwebp-base 1.2.0 h0d85af4_2",
    "libxml2 2.9.12 h93ec3fd_0",
    "llvm-openmp 11.1.0 hda6cdc1_1",
    "llvm-tools 11.1.0 hd011deb_2",
    "lz4-c 1.9.3 h046ec9c_0",
    "make 4.3 h22f3db7_1",
    "mpc 1.1.0 ha57cd0f_1009",
    "mpfr 4.0.2 h72d8aaf_1",
    "ncurses 6.2 h2e338ed_4",
    "openssl 1.1.1k h0d85af4_0",
    "pango 1.48.5 ha05cd14_0",
    "pcre 8.44 hb1e8313_0",
    "pcre2 10.36 h5cf9962_1",
    "pixman 0.40.0 hbcb3906_0",
    "r-base 4.1.0 h4d23a9d_1",
    "r-rcpp 1.0.6 r41h9951f98_0",
    "r-snowballc 0.7.0 r41h28b5c78_1",
    "r-stringi 1.6.2 r41hd783aa8_0",
    "readline 8.1 h05e3726_0",
    "tapi 1100.0.11 h9ce4665_0",
    "tk 8.6.10 h0419947_1",
    "tktable 2.10 h49f0cf7_3",
    "xz 5.2.5 haf1e3a3_1",
    "zlib 1.2.11 h7795811_1010",
    "zstd 1.5.0 h582d3a0_0"
   ],
   "run": [
    "libcxx >=11.1.0",
    "r-base >=4.1,<4.2.0a0",
    "r-rcpp >=0.12.3",
    "r-snowballc >=0.5.1",
    "r-stringi >=1.0.1"
   ]
  },
  "source": {
   "fn": "tokenizers_0.2.1.tar.gz",
   "sha256": "28617cdc5ddef5276abfe14a2642999833322b6c34697de1d4e9d6dc7670dd00",
   "url": [
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('tokenizers')\""
   ]
  }
 },
 "version": "0.2.1"
}